{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addd78f7-c8e6-48f9-ad26-ffc47ca3fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, sys, os, io, lmdb\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294982da",
   "metadata": {},
   "source": [
    "Add the root working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bcaf619-5bcd-481f-b715-e72e75196ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Downloads\\text-super-resolution-network\n"
     ]
    }
   ],
   "source": [
    "PARENT_DIR = Path.cwd().parent\n",
    "sys.path.insert(0, str(PARENT_DIR))\n",
    "\n",
    "print(PARENT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5952b4e-ba70-4030-a5c3-299b13becd3a",
   "metadata": {},
   "source": [
    "Define the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ded38f1-3d0a-4ee3-89ac-5e4dcede3e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Downloads\\text-super-resolution-network\\dataset\\TextZoom\\train1\n",
      "C:\\Users\\User\\Downloads\\text-super-resolution-network\\dataset\\TextZoom\\train2\n",
      "C:\\Users\\User\\Downloads\\text-super-resolution-network\\dataset\\TextZoom\\test\\easy\n",
      "C:\\Users\\User\\Downloads\\text-super-resolution-network\\dataset\\TextZoom\\test\\medium\n",
      "C:\\Users\\User\\Downloads\\text-super-resolution-network\\dataset\\TextZoom\\test\\hard\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import DATASET_DIR, get_device, set_seed\n",
    "from utils.ssim_psnr import calculate_psnr, SSIM\n",
    "from utils.metrics import get_str_list, Accuracy\n",
    "from utils.labelmaps import get_vocabulary\n",
    "from models.recognizer.tps_spatial_transformer import TPSSpatialTransformer\n",
    "from models.recognizer.stn_head import STNHead\n",
    "from models.recognizer.recognizer_builder import RecognizerBuilder\n",
    "\n",
    "DATASET = \"TextZoom\"\n",
    "TRAIN1_DIR = os.path.join(DATASET_DIR, DATASET, \"train1\")\n",
    "TRAIN2_DIR = os.path.join(DATASET_DIR, DATASET, \"train2\")\n",
    "TEST1_DIR = os.path.join(DATASET_DIR, DATASET, \"test\", \"easy\")\n",
    "TEST2_DIR = os.path.join(DATASET_DIR, DATASET, \"test\", \"medium\")\n",
    "TEST3_DIR = os.path.join(DATASET_DIR, DATASET, \"test\", \"hard\")\n",
    "\n",
    "print(TRAIN1_DIR)\n",
    "print(TRAIN2_DIR)\n",
    "print(TEST1_DIR)\n",
    "print(TEST2_DIR)\n",
    "print(TEST3_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2379fd4-75f5-4f7b-9910-d199c1e9537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "VOC_TYPE = \"all\"\n",
    "set_seed(SEED)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c57ea-d5e0-4a5b-949c-3b6f3b913f66",
   "metadata": {},
   "source": [
    "### Build TextZoom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e57595-f024-49b3-abb1-6fed429cf179",
   "metadata": {},
   "source": [
    "Define the `TextZoomDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c523e552-9201-415b-bc5f-164064545338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import filter_str\n",
    "\n",
    "class TextZoomDataset(Dataset):\n",
    "    def __init__(self, data_dir=None, voc_type=\"upper\", max_len=33):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.voc_type = voc_type\n",
    "        self.max_len = max_len\n",
    "\n",
    "        env = lmdb.open(self.data_dir, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        if not env:\n",
    "            print('Cannot create lmdb from %s' % (data_dir))\n",
    "            sys.exit(0)\n",
    "\n",
    "        with env.begin(write=False) as txn:\n",
    "            num_samples = int(txn.get(b'num-samples'))\n",
    "            self.num_samples = num_samples\n",
    "        env.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, index):       \n",
    "        index += 1\n",
    "        env = lmdb.open(self.data_dir, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        \n",
    "        with env.begin(write=False) as txn:\n",
    "            hr_key = b'image_hr-%09d' % index\n",
    "            lr_key = b'image_lr-%09d' % index\n",
    "            label_key = b'label-%09d' % index\n",
    "\n",
    "            hr_buffer = txn.get(hr_key)\n",
    "            lr_buffer = txn.get(lr_key)\n",
    "            label_buffer = txn.get(label_key)\n",
    "\n",
    "            # error handling: if data is missing\n",
    "            if lr_buffer is None or hr_buffer is None or label_buffer is None:\n",
    "                return self.__getitem__(index)\n",
    "\n",
    "            # convert Bytes to PIL Image\n",
    "            img_hr = Image.open(io.BytesIO(hr_buffer)).convert('RGB')\n",
    "            img_lr = Image.open(io.BytesIO(lr_buffer)).convert('RGB')\n",
    "\n",
    "            # decode label\n",
    "            label = str(label_buffer.decode())\n",
    "            label = filter_str(label, self.voc_type)\n",
    "\n",
    "            return img_hr, img_lr, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbceee9-2234-48f8-9f96-376ca5c24f4b",
   "metadata": {},
   "source": [
    "Define the `ResizeNormalize` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c19e8d-dd2c-4aa8-a482-efe9919c4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resizeNormalize(object):\n",
    "    def __init__(self, size, mask=False, interpolation=Image.BICUBIC):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.mask = mask\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = img.resize(self.size, self.interpolation)\n",
    "        img_tensor = self.toTensor(img)\n",
    "        if self.mask:\n",
    "            mask = img.convert('L')\n",
    "            thres = np.array(mask).mean()\n",
    "            mask = mask.point(lambda x: 0 if x > thres else 255)\n",
    "            mask = self.toTensor(mask)\n",
    "            img_tensor = torch.cat((img_tensor, mask), 0)\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9c68d-6a4e-48c0-bd3e-61d46e3364ee",
   "metadata": {},
   "source": [
    "Define the `AlignCollate` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f8f5b6-8a0f-48f1-bf05-546002aa972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignCollate():\n",
    "    def __init__(self, imgH=64, imgW=256, down_sample_scale=4, keep_ratio=False, min_ratio=1, mask=False):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio = keep_ratio\n",
    "        self.min_ratio = min_ratio\n",
    "        self.down_sample_scale = down_sample_scale\n",
    "        self.mask = mask\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        img_hr, img_lr, label = zip(*batch)\n",
    "        imgH = self.imgH\n",
    "        imgW = self.imgW\n",
    "        transform = resizeNormalize((imgW, imgH), self.mask)\n",
    "        transform2 = resizeNormalize((imgW // self.down_sample_scale, imgH // self.down_sample_scale), self.mask)\n",
    "        img_hr = [transform(image) for image in img_hr]\n",
    "        img_hr = torch.cat([t.unsqueeze(0) for t in img_hr], 0)\n",
    "\n",
    "        img_lr = [transform2(image) for image in img_lr]\n",
    "        img_lr = torch.cat([t.unsqueeze(0) for t in img_lr], 0)\n",
    "\n",
    "        return img_hr, img_lr, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7eceed-c296-4474-b9ac-d6ae686e6500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bc4f8b8-70fd-45bb-9146-aaf6361e6671",
   "metadata": {},
   "source": [
    "### Build TSRN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47666db8-9844-4d81-9f1e-84b56ecf114b",
   "metadata": {},
   "source": [
    "Define the `GRUBlock` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ad43b1-a2d7-4a57-aacf-4f63eb7eee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GRUBlock, self).__init__()\n",
    "        assert out_channels % 2 == 0\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "        self.gru = nn.GRU(out_channels, out_channels // 2, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        b = x.size()\n",
    "        x = x.view(b[0] * b[1], b[2], b[3])\n",
    "        x, _ = self.gru(x)\n",
    "        # x = self.gru(x)[0]\n",
    "        x = x.view(b[0], b[1], b[2], b[3])\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772738c5-2686-41e4-a1e4-020927a71ed7",
   "metadata": {},
   "source": [
    "Define the `MISH` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f4dfa7-cb18-40ea-8400-f58b3eaf8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MISH(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(MISH, self).__init__()\n",
    "        self.activated = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activated:\n",
    "            x = x * (torch.tanh(F.softplus(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4295c523-c6bf-4b8b-bd39-a61c69159a61",
   "metadata": {},
   "source": [
    "Define the `RecurrentResidualBLock` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee200fa2-6272-4aa5-93cd-0c342662e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(RecurrentResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.gru1 = GRUBlock(channels, channels)\n",
    "        # self.prelu = nn.ReLU()\n",
    "        self.prelu = MISH()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.gru2 = GRUBlock(channels, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.bn1(residual)\n",
    "        residual = self.prelu(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        residual = self.bn2(residual)\n",
    "        residual = self.gru1(residual.transpose(-1, -2)).transpose(-1, -2)\n",
    "        # residual = self.non_local(residual)\n",
    "\n",
    "        return self.gru2(x + residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a60e79-58ad-47cf-80cb-952911f9b326",
   "metadata": {},
   "source": [
    "Define the `UpsampleBlock` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb8e2024-9f80-40e8-bf6b-93ad5e9870bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleBLock(nn.Module):\n",
    "    def __init__(self, in_channels, up_scale):\n",
    "        super(UpsampleBLock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
    "        # self.prelu = nn.ReLU()\n",
    "        self.prelu = MISH()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.prelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509c967-9b2f-456b-867b-1d2451a35766",
   "metadata": {},
   "source": [
    "Define the `TSRN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "144ecaec-6858-4c6e-b402-79475b207788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSRN(nn.Module):\n",
    "    def __init__(self, scale_factor=2, width=128, height=32, STN=False, srb_nums=5, mask=True, hidden_units=32):\n",
    "        super(TSRN, self).__init__()\n",
    "        in_planes = 3\n",
    "        if mask:\n",
    "            in_planes = 4\n",
    "        assert math.log(scale_factor, 2) % 1 == 0\n",
    "        upsample_block_num = int(math.log(scale_factor, 2))\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, 2*hidden_units, kernel_size=9, padding=4),\n",
    "            nn.PReLU()\n",
    "            # nn.ReLU()\n",
    "        )\n",
    "        self.srb_nums = srb_nums\n",
    "        for i in range(srb_nums):\n",
    "            setattr(self, 'block%d' % (i + 2), RecurrentResidualBlock(2*hidden_units))\n",
    "\n",
    "        setattr(self, 'block%d' % (srb_nums + 2),\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(2*hidden_units, 2*hidden_units, kernel_size=3, padding=1),\n",
    "                    nn.BatchNorm2d(2*hidden_units)\n",
    "                ))\n",
    "        \n",
    "        # self.non_local = NonLocalBlock2D(64, 64)\n",
    "        block_ = [UpsampleBLock(2*hidden_units, 2) for _ in range(upsample_block_num)]\n",
    "        block_.append(nn.Conv2d(2*hidden_units, in_planes, kernel_size=9, padding=4))\n",
    "        setattr(self, 'block%d' % (srb_nums + 3), nn.Sequential(*block_))\n",
    "        self.tps_inputsize = [32, 64]\n",
    "        tps_outputsize = [height//scale_factor, width//scale_factor]\n",
    "        num_control_points = 20\n",
    "        tps_margins = [0.05, 0.05]\n",
    "        self.stn = STN\n",
    "        if self.stn:\n",
    "            self.tps = TPSSpatialTransformer(\n",
    "                output_image_size=tuple(tps_outputsize),\n",
    "                num_control_points=num_control_points,\n",
    "                margins=tuple(tps_margins))\n",
    "\n",
    "            self.stn_head = STNHead(\n",
    "                in_planes=in_planes,\n",
    "                num_ctrlpoints=num_control_points,\n",
    "                activation='none')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embed()\n",
    "        if self.stn and self.training:\n",
    "            x = F.interpolate(x, self.tps_inputsize, mode='bilinear', align_corners=True)\n",
    "            _, ctrl_points_x = self.stn_head(x)\n",
    "            x, _ = self.tps(x, ctrl_points_x)\n",
    "        block = {'1': self.block1(x)}\n",
    "        for i in range(self.srb_nums + 1):\n",
    "            block[str(i + 2)] = getattr(self, 'block%d' % (i + 2))(block[str(i + 1)])\n",
    "\n",
    "        block[str(self.srb_nums + 3)] = getattr(self, 'block%d' % (self.srb_nums + 3)) \\\n",
    "            ((block['1'] + block[str(self.srb_nums + 2)]))\n",
    "        output = torch.tanh(block[str(self.srb_nums + 3)])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af065ffd-7e21-4ce2-a7a4-24910c002f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a64209-b91f-4679-9676-fed144fdccb7",
   "metadata": {},
   "source": [
    "### Build Aster architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa85438-6ab2-4edc-85e0-260999ccde00",
   "metadata": {},
   "source": [
    "Define `AsterInfo` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b4f9188-37b1-4aee-b5bd-5db637f7282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsterInfo(object):\n",
    "    def __init__(self, voc_type):\n",
    "        super(AsterInfo, self).__init__()\n",
    "        self.voc_type = voc_type\n",
    "        assert voc_type in ['digit', 'lower', 'upper', 'all']\n",
    "        self.EOS = 'EOS'\n",
    "        self.max_len = 100\n",
    "        self.PADDING = 'PADDING'\n",
    "        self.UNKNOWN = 'UNKNOWN'\n",
    "        self.voc = get_vocabulary(voc_type, EOS=self.EOS, PADDING=self.PADDING, UNKNOWN=self.UNKNOWN)\n",
    "        self.char2id = dict(zip(self.voc, range(len(self.voc))))\n",
    "        self.id2char = dict(zip(range(len(self.voc)), self.voc))\n",
    "        self.rec_num_classes = len(self.voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87cc76-e1bf-4d7b-9a2d-a2791879220d",
   "metadata": {},
   "source": [
    "Define the `aster_init` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10511726-c7a2-466d-ac9b-010611b30752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aster_init():\n",
    "    ASTER_PATH = PARENT_DIR.joinpath(\"aster.pth\")\n",
    "    aster_info = AsterInfo(VOC_TYPE)\n",
    "    aster = RecognizerBuilder(arch='ResNet_ASTER', rec_num_classes=aster_info.rec_num_classes,\n",
    "                                         sDim=512, attDim=512, max_len_labels=aster_info.max_len,\n",
    "                                         eos=aster_info.char2id[aster_info.EOS], STN_ON=True)\n",
    "    aster.load_state_dict(torch.load(ASTER_PATH)['state_dict'])\n",
    "    aster = aster.to(device)\n",
    "\n",
    "    for p in aster.parameters():\n",
    "        p.requires_grad = False\n",
    "    aster.eval()\n",
    "    \n",
    "    print(f\"load pred_trained aster model from %s\" % {ASTER_PATH})\n",
    "    return aster, aster_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a89b3a-6a75-4003-9672-7093abd0acc8",
   "metadata": {},
   "source": [
    "Define the `parse_aster_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8775187-87f7-4d1f-801b-3368a8ee85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_aster_data(aster_info, imgs_input):\n",
    "    input_dict = {}\n",
    "    images_input = imgs_input.to(device)\n",
    "    input_dict['images'] = images_input * 2 - 1\n",
    "    batch_size = images_input.shape[0]\n",
    "    input_dict['rec_targets'] = torch.LongTensor(batch_size, aster_info.max_len).fill_(1).to(device)\n",
    "    input_dict['rec_lengths'] = torch.LongTensor([aster_info.max_len] * batch_size).to(device)\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e6ae8-50ee-4617-b243-2288404c3c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "790a2b6a-dbf8-4e6c-8540-cd7416e49f22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc285f-96a4-4535-9b93-718cf3a75b06",
   "metadata": {},
   "source": [
    "Define the `GradientPriorLoss` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf8847b8-fe0e-426d-93c2-ff0809ac213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientPriorLoss(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(GradientPriorLoss, self).__init__()\n",
    "        self.func = nn.L1Loss()\n",
    "\n",
    "    def forward(self, out_images, target_images):\n",
    "        map_out = self.gradient_map(out_images)\n",
    "        map_target = self.gradient_map(target_images)\n",
    "        return self.func(map_out, map_target)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_map(x):\n",
    "        batch_size, channel, h_x, w_x = x.size()\n",
    "        r = F.pad(x, (0, 1, 0, 0))[:, :, :, 1:]\n",
    "        l = F.pad(x, (1, 0, 0, 0))[:, :, :, :w_x]\n",
    "        t = F.pad(x, (0, 0, 1, 0))[:, :, :h_x, :]\n",
    "        b = F.pad(x, (0, 0, 0, 1))[:, :, 1:, :]\n",
    "        xgrad = torch.pow(torch.pow((r - l) * 0.5, 2) + torch.pow((t - b) * 0.5, 2)+1e-6, 0.5)\n",
    "        return xgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f81ddc-811b-4dd4-a90b-40876c2eb817",
   "metadata": {},
   "source": [
    "Define the `ImageLoss` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "295c04f4-d229-48d7-ad52-d68c238e140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLoss(nn.Module):\n",
    "    def __init__(self, gradient=True, loss_weight=[20, 1e-4]):\n",
    "        super(ImageLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        if gradient:\n",
    "            self.GPLoss = GradientPriorLoss()\n",
    "        self.gradient = gradient\n",
    "        self.loss_weight = loss_weight\n",
    "\n",
    "    def forward(self, out_images, target_images):\n",
    "        if self.gradient:\n",
    "            loss = self.loss_weight[0] * self.mse(out_images, target_images) + \\\n",
    "                   self.loss_weight[1] * self.GPLoss(out_images[:, :3, :, :], target_images[:, :3, :, :])\n",
    "        else:\n",
    "            loss = self.loss_weight[0] * self.mse(out_images, target_images)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf197b-adbe-4542-a6a9-0eae749d2392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e5e8af-6e3f-45f8-80b0-b0fd3b00b583",
   "metadata": {},
   "source": [
    "### Training phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fe7c0-2048-4fe5-9edc-1d5b03ea677e",
   "metadata": {},
   "source": [
    "Setting up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c98273e2-a0c4-4cb2-8c69-c4727fd0bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "BETA1 = 0.5\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aa3a87e-bbbc-4075-ad23-31489c248fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "align_collate = AlignCollate(imgH=32, imgW=128, down_sample_scale=2, mask=False)\n",
    "\n",
    "# load the training dataset\n",
    "train1_dataset, train2_dataset = TextZoomDataset(TRAIN1_DIR), TextZoomDataset(TRAIN2_DIR)\n",
    "train_dataset = ConcatDataset([train1_dataset, train2_dataset])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, collate_fn=align_collate)\n",
    "\n",
    "# load the testing dataset\n",
    "test1_dataset, test2_dataset, test3_dataset = TextZoomDataset(TEST1_DIR), TextZoomDataset(TEST2_DIR), TextZoomDataset(TEST3_DIR)\n",
    "val_dataset = ConcatDataset([test1_dataset, test2_dataset, test3_dataset])\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, collate_fn=align_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5a16235-4f8c-436b-8ff4-c024688f7e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pred_trained aster model from {WindowsPath('C:/Users/User/Downloads/text-super-resolution-network/aster.pth')}\n"
     ]
    }
   ],
   "source": [
    "model = TSRN(scale_factor=2, width=128, height=32, STN=True, mask=False, srb_nums=5, hidden_units=32).to(device)\n",
    "aster, aster_info = aster_init()\n",
    "criterion = ImageLoss(gradient=True, loss_weight=[1, 1e-4])\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(BETA1, 0.999))\n",
    "\n",
    "calculate_ssim = SSIM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c21c1-c506-4d56-87c9-f7a8aa4982eb",
   "metadata": {},
   "source": [
    "Start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff839e-1e81-4d9e-89c7-f7917dd1fefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating epoch 01:  26%|████████████▌                                    | 9/35 [02:30<07:00, 16.17s/it, loss=1.6260]"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # ------------ training phrase ------------\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    batch_iterator = tqdm(train_loader, leave=False, desc=f'Processing epoch {epoch+1:02d}')\n",
    "\n",
    "    for j, batch in (enumerate(batch_iterator)):\n",
    "        img_hr, img_lr, label = batch\n",
    "        img_hr = img_hr.to(device)\n",
    "        img_lr = img_lr.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        img_sr = model(img_lr)\n",
    "        loss = criterion(img_sr, img_hr).mean() * 100\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "\n",
    "        # update statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # update progress bar\n",
    "        batch_iterator.set_postfix(loss=f\"{loss.item():.5f}\", lr=f\"{optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    # calculate average metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # ------------ validation phrase ------------\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    n_correct, total_images = 0, 0\n",
    "    psnr_list, ssim_list = [], []\n",
    "    batch_iterator = tqdm(val_loader, leave=False, desc=f'Validating epoch {epoch+1:02d}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in batch_iterator:\n",
    "            img_hr, img_lr, label = batch\n",
    "            batch_size = img_lr.shape[0]\n",
    "            img_hr = img_hr.to(device)\n",
    "            img_lr = img_lr.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            img_sr = model(img_lr)\n",
    "            loss = criterion(img_sr, img_hr).mean() * 100\n",
    "\n",
    "            # calculate image quality metrics\n",
    "            psnr_list.append(calculate_psnr(img_sr, img_hr))\n",
    "            ssim_list.append(calculate_ssim(img_sr, img_hr))\n",
    "\n",
    "            # prepare data for ASTER text recognition\n",
    "            aster_dict_sr = parse_aster_data(aster_info, img_sr[:, :3, :, :])\n",
    "            aster_dict_lr = parse_aster_data(aster_info, img_lr[:, :3, :, :])\n",
    "\n",
    "            # get text predictions\n",
    "            aster_output_lr = aster(aster_dict_lr)\n",
    "            aster_output_sr = aster(aster_dict_sr)\n",
    "\n",
    "            pred_rec_lr = aster_output_lr['output']['pred_rec']\n",
    "            pred_rec_sr = aster_output_sr['output']['pred_rec']\n",
    "\n",
    "            pred_str_lr, _ = get_str_list(pred_rec_lr, aster_dict_lr['rec_targets'], dataset=aster_info)\n",
    "            pred_str_sr, _ = get_str_list(pred_rec_sr, aster_dict_sr['rec_targets'], dataset=aster_info)\n",
    "\n",
    "            # calculate text recognition accuracy\n",
    "            for pred, target in zip(pred_str_sr, label):\n",
    "                if pred == filter_str(target, VOC_TYPE):\n",
    "                    n_correct += 1\n",
    "\n",
    "            # calculate losses (for logging only)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            total_images += batch_size\n",
    "\n",
    "            # update progress bar\n",
    "            batch_iterator.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            \n",
    "    # calculate average metrics\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    psnr_avg = sum(psnr_list) / len(psnr_list)\n",
    "    ssim_avg = sum(ssim_list) / len(ssim_list)\n",
    "    accuracy = n_correct / total_images\n",
    "    \n",
    "    # print results\n",
    "    print(f'[{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}]\\t'\n",
    "          f'EPOCH: {epoch+1}/{EPOCHS}')\n",
    "    print(f'ASTER accuracy: {accuracy * 100:.2f}%')\n",
    "    print(f'train_loss {train_loss:.5f} | val_loss {val_loss:.5f}')\n",
    "    print(f'PSNR {psnr_avg.item():.2f} | SSIM {ssim_avg.item():.4f}')\n",
    "\n",
    "    metrics_list.append({\n",
    "        'accuracy': round(accuracy, 4),\n",
    "        'psnr_avg': round(psnr_avg.item(), 6),\n",
    "        'ssim_avg': round(ssim_avg.item(), 6),\n",
    "        'psnr': psnr_list,\n",
    "        'ssim': ssim_list\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
